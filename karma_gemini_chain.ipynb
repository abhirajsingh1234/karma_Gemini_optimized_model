{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c8b9e8-5684-4389-9242-32d0d24feda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import dependencies\n",
    "\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import ast\n",
    "import pandas\n",
    "import sys\n",
    "import time\n",
    "import requests  \n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from datasets import load_dataset\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bdd5c4d-de07-49f9-92c6-db402d3b774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset from HF and Convert to Pandas DataFrame\n",
    "\n",
    "# dataset = load_dataset(\"AbhirajSinghRajpurohit/karma_LLM_model_Dataset\")\n",
    "# qa_pairs = dataset[\"train\"].to_pandas()\n",
    "\n",
    "# data=pandas.read_csv('Content_Storage_df.csv')\n",
    "# Content_Storage_df = pandas.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e385d4-1d7b-42c1-bcf4-44bbdc41f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = chroma_client.get_collection(\"vector_embeddings\")\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "history=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "440ccd1b-59c6-43f7-ae2c-e721312f2821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 8190,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "retrieve_grader_1 = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash-8b\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"\"\"You are a grader assessing the relevance of a retrieved document to a user question. \n",
    "                        Give a binary score 'yes' or 'no' to indicate whether the document is relevant.\n",
    "                        Provide the binary score as JSON with a single key 'score'.\n",
    "                        input format is 'question : question , document : document'. \"\"\"\n",
    ")\n",
    "web_search_1_5= genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash-8b\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"\"\"You are an AI assistant specialized in retrieving spiritual and positive context from the Garud Puran.  \n",
    "                        Use the given question to extract a meaningful and relevant context that aligns with the teachings of the Garud Puran.  \n",
    "                        \n",
    "                        If the user asks about their **karma, actions, or deeds**, retrieve context that explains both the **pros and cons** based on what will happen in **Swarg (heaven) and Nark (hell)** according to the Garud Puran and Sanatan Dharma.  \n",
    "                        Ensure the context clearly describes the **specific rewards in Swarg** for good deeds, bringing peace and happiness, and the **specific punishments in Nark** for bad deeds, leading to suffering and atonement.  \n",
    "                        The context should also highlight the **emotional and spiritual consequences** of one's actions, helping the user understand the **joy behind righteousness** and the **pain behind sinful acts**.  \n",
    "                        \n",
    "                        For **general queries**, retrieve an example from the Garud Puran that illustrates the concept in a way that makes it relatable and insightful.  \n",
    "                        \n",
    "                        Ensure the context is spiritually uplifting, guiding the user toward self-reflection and improvement, while keeping it concise (maximum of three paragraphs).  \n",
    "                        Each retrieved context should be **unique**, providing fresh insights or varying perspectives while staying true to the scripture's teachings.  \n",
    "                        Use **simple and clear English**, avoiding complex words, so the response is easy to understand for all users.  \n",
    "\n",
    "                        \"\"\"\n",
    ")\n",
    "answer_generator_2 = genai.GenerativeModel(\n",
    "  model_name=\"gemini-2.0-pro-exp-02-05\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction= \"\"\"You are an AI assistant designed for question-answering tasks based on the Garud Puran.\n",
    "                        \n",
    "                        Use the retrieved context to generate an accurate and spiritually meaningful response.  \n",
    "                        Ensure that the answer aligns with the teachings of the Garud Puran and maintains a positive and enlightening tone.  \n",
    "                        \n",
    "                        If the user asks about their **karma, actions, or deeds**, include both the **pros and cons** based on what will happen in **Swarg (heaven) and Nark (hell)** according to the Garud Puran and Sanatan Dharma.\n",
    "                        Clearly mention the **specific rewards in Swarg** for good deeds and the **specific punishments in Nark** for bad deeds, as described in the scriptures.  \n",
    "\n",
    "                        For **general queries**, provide a relevant example from the Garud Puran to illustrate the concept effectively.\n",
    "\n",
    "                        ### **Response Structure:**  \n",
    "                        1Ô∏è‚É£ **First, directly answer the question.**  \n",
    "                           - Describe the **specific punishments in Nark (hell)** or **specific rewards in Swarg (heaven)** based on their karma, as per the Garud Puran.  \n",
    "                           - Help the user understand the **joy and blessings** their good deeds bring and the **pain and suffering** caused by their bad deeds.  \n",
    "                        \n",
    "                        2Ô∏è‚É£ **Then, provide guidance on how to resolve or approach the problem from a spiritual perspective.**  \n",
    "                           - Explain how the user can **overcome negative karma** through righteous actions, devotion, and self-correction.  \n",
    "                           - Offer spiritual remedies or practices from the Garud Puran to seek divine grace and move toward a better path.  \n",
    "                        \n",
    "                        3Ô∏è‚É£ **Finally, include a general spiritual thought or insight related to the question.**  \n",
    "                           - Inspire the user with a **positive message about karma** and the cycle of cause and effect.  \n",
    "                           - Reinforce that **good deeds lead to inner peace and happiness**, while **wrong actions create suffering, but redemption is always possible through wisdom and self-improvement**.  \n",
    " \n",
    "                        \n",
    "                        Keep your response concise, with a maximum of three sentences.  \n",
    "                        End with a positive thought related to the question, inspired by the spiritual wisdom of the Garud Puran.  \n",
    "                        Use **simple and clear English**, avoiding complex words, so the response is easy to understand for all users.\n",
    "                        **Respond in the same language in which the user asks the question.**  \n",
    "                        \"\"\"\n",
    "  # system_instruction=\"\"\"You are an AI assistant designed for question-answering tasks based on the Garud Puran.  \n",
    "\n",
    "  #                       Use the retrieved context to generate an accurate and spiritually meaningful response.  \n",
    "  #                       Ensure that the answer aligns with the teachings of the Garud Puran and maintains a positive and enlightening tone.  \n",
    "                        \n",
    "  #                       ### **Handling Different Types of Questions:**  \n",
    "  #                       üü¢ **For karma-related queries**  \n",
    "  #                       - Include both the **pros and cons** of their actions based on **Swarg (heaven) and Nark (hell)** as per the Garud Puran and Sanatan Dharma.  \n",
    "  #                       - Clearly describe **specific rewards in Swarg** for good deeds and **specific punishments in Nark** for bad deeds.  \n",
    "                        \n",
    "  #                       üü¢ **For general queries about the Garud Puran**  \n",
    "  #                       - Provide an **explanation or summary of relevant teachings** from the scripture.  \n",
    "  #                       - If possible, include **a related story, parable, or example** from the Garud Puran to illustrate the concept effectively.  \n",
    "  #                       - Ensure that the response remains **uplifting, thought-provoking, and spiritually insightful**.  \n",
    "                        \n",
    "  #                       ### **Response Structure:**  \n",
    "  #                       1Ô∏è‚É£ **First, directly answer the question.**  \n",
    "  #                          - If the question is **karma-related**, explain the **Swarg/Nark consequences**.  \n",
    "  #                          - If the question is **general**, provide a **relevant explanation or story** from the Garud Puran.  \n",
    "\n",
    "  #                       2Ô∏è‚É£ **Then, offer guidance or wisdom related to the topic.**  \n",
    "  #                          - Provide insights into how the user can apply the knowledge in their life.  \n",
    "  #                          - If the question is about karma, suggest ways to **correct negative karma**.  \n",
    "  #                          - If it is a general query, explain **the deeper meaning or lesson** behind the concept.  \n",
    "                        \n",
    "  #                       3Ô∏è‚É£ **Finally, include a general spiritual thought or insight.**  \n",
    "  #                          - Inspire the user with a **positive message** from the Garud Puran.  \n",
    "  #                          - Reinforce **moral, ethical, and spiritual values** in a way that resonates with them.  \n",
    "                        \n",
    "                        # Keep your response concise, with a maximum of three sentences.  \n",
    "                        # End with a positive thought related to the question, inspired by the spiritual wisdom of the Garud Puran.  \n",
    "                        # Use **simple and clear English**, avoiding complex words, so the response is easy to understand for all users.  \n",
    "                        # **Respond in the same language in which the user asks the question.**  \n",
    "                        # \"\"\"\n",
    "  \n",
    ")\n",
    "hallucination_detection_3 = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash-8b\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"\"\"You are verifying whether the model-generated answer is factually correct based on the provided context.  \n",
    "                        If the answer includes information not found in the context, classify it as hallucinated.  \n",
    "                        Respond with a JSON object containing a single key `\"hallucination\"`, with a value of `\"yes\"` or `\"no\"`.  \n",
    "                        \n",
    "                        Output Format:  \n",
    "                        {\n",
    "                          \"hallucination\": \"yes\"  // If the answer contains hallucinated information  \n",
    "                        }  \n",
    "                        {\n",
    "                          \"hallucination\": \"no\"   // If the answer is fully supported by the context  \n",
    "                        }   \"\"\"\n",
    ")\n",
    "question_resolving_detection_4 = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash-8b\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"\"\"You are a grader evaluating whether an answer is useful in resolving the given question.  \n",
    "                        Assess if the answer is relevant, clear, and provides sufficient information to address the question.  \n",
    "                        Respond with a JSON object containing a single key `\"score\"`, with a value of `\"yes\"` or `\"no\"`.  \n",
    "                        \n",
    "                        Input Format:  \n",
    "                        question: {question}, answer: {answer}  \n",
    "                        \n",
    "                        Output Format:  \n",
    "                        {\n",
    "                          \"score\": \"yes\"  // If the answer is useful  \n",
    "                        }  \n",
    "                        {\n",
    "                          \"score\": \"no\"   // If the answer is not useful  \n",
    "                        }  \n",
    "                        \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c7deec-3600-4fe5-ae96-5fd5f4f5a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, top_k=3):\n",
    "    embedding_fn = embedding_functions.DefaultEmbeddingFunction()\n",
    "    question_embedding = embedding_fn([question])[0]\n",
    "\n",
    "    # Retrieve top-k matching documents\n",
    "    results = collection.query(\n",
    "        query_embeddings=[question_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    if results[\"documents\"]:\n",
    "        # print(results[\"documents\"])\n",
    "        flat_documents = [doc for sublist in results[\"documents\"] for doc in sublist]\n",
    "        return \" \".join(flat_documents) if flat_documents else \"No relevant context found.\"\n",
    "    \n",
    "    return \"No relevant context found.\"\n",
    "\n",
    "def retrieve_grader_function(question):\n",
    "    \n",
    "    chat_session = retrieve_grader_1.start_chat(\n",
    "                history=history\n",
    "            )\n",
    "    \n",
    "    response = chat_session.send_message(question)\n",
    "    \n",
    "    model_response=response.text\n",
    "    return model_response\n",
    "\n",
    "# Web search function \n",
    "def web_search(query, num_results=3):\n",
    "    \n",
    "    chat_session = web_search_1_5.start_chat(\n",
    "                history=history\n",
    "            )\n",
    "    \n",
    "    response = chat_session.send_message(query)\n",
    "    \n",
    "    model_response=response.text\n",
    "    return model_response\n",
    "    \n",
    "\n",
    "def answer_generator_function(question):\n",
    "    \n",
    "    chat_session = answer_generator_2.start_chat(\n",
    "                history=history\n",
    "            )\n",
    "    \n",
    "    response = chat_session.send_message(question)\n",
    "    \n",
    "    model_response=response.text\n",
    "    return model_response\n",
    "    \n",
    "def hallucination_detection_function(question):\n",
    "    \n",
    "    chat_session = hallucination_detection_3.start_chat(\n",
    "                history=history\n",
    "            )\n",
    "    \n",
    "    response = chat_session.send_message(question)\n",
    "    \n",
    "    model_response=response.text\n",
    "    return model_response\n",
    "def question_resolving_detection_function(question):\n",
    "    \n",
    "    chat_session = question_resolving_detection_4.start_chat(\n",
    "                history=history\n",
    "            )\n",
    "    \n",
    "    response = chat_session.send_message(question)\n",
    "    \n",
    "    model_response=response.text\n",
    "    return model_response\n",
    "\n",
    "\n",
    "#Full Path\n",
    "def Full_Flow(question):\n",
    "    flag=0\n",
    "    while True:\n",
    "        if flag==0:\n",
    "            document = retrieve_context(question)\n",
    "            model_input = f\"question : {question} , document : {document}\"\n",
    "            output = retrieve_grader_function(model_input)\n",
    "            \n",
    "            if 'yes' in output:\n",
    "                print('document found in database')\n",
    "            elif 'no' in output:\n",
    "                print('searching web.....')\n",
    "                print('document found on web.....')\n",
    "                document = web_search(question)\n",
    "        elif flag==1:\n",
    "            print('searching web.....')\n",
    "            print('document found on web.....')\n",
    "            document = web_search(question)\n",
    "        # print(document)\n",
    "        while True:   \n",
    "            #Generation of answer based on context\n",
    "            model_input = f\"question : {question},context : {document}\"\n",
    "            answer = answer_generator_function(model_input)\n",
    "            print('answer fetched from document')\n",
    "        \n",
    "            #Hallucination detection to check the correctness of answer\n",
    "            hallucination_check_input = f\"context : {document}, answer : {answer}\"\n",
    "            hallucination_output = hallucination_detection_function(hallucination_check_input)\n",
    "            if 'yes' in hallucination_output:\n",
    "                print('hallucination detected.')\n",
    "                print('regenerating the answer....\\n')\n",
    "                continue\n",
    "            elif 'no' in hallucination_output:\n",
    "                print('no hallucination detected')\n",
    "                question_resolver_input = f' question: {question}, answer: {answer}'\n",
    "                question_resolver_output = question_resolving_detection_function(question_resolver_input)\n",
    "                # print(question_resolver_output)\n",
    "                break\n",
    "            else: return None\n",
    "        if 'no' in question_resolver_output:\n",
    "            print('the generated answer do not resolve the query\\n')\n",
    "            print('searching the relevant document again.....\\n')\n",
    "            flag=1\n",
    "            continue\n",
    "            \n",
    "        elif 'yes' in question_resolver_output:\n",
    "            print('generated answer will resolve the query\\n\\n')\n",
    "            return 'answer :'+answer\n",
    "        else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123ddd12-ec7f-4a99-9ad6-fe64780a3802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching web.....\n",
      "document found on web.....\n",
      "answer fetched from document\n",
      "no hallucination detected\n",
      "the generated answer do not resolve the query\n",
      "\n",
      "searching the relevant document again.....\n",
      "\n",
      "searching web.....\n",
      "document found on web.....\n",
      "answer fetched from document\n",
      "no hallucination detected\n",
      "generated answer will resolve the query\n",
      "\n",
      "\n",
      "answer :Arjuna asked Krishna about the moral consequences of his actions in the war, questioning the righteousness of engaging in such violence. Krishna explained that war, while violent, can be necessary to maintain justice and order, emphasizing the importance of the right motive and acceptance of outcomes. Remember, aligning your actions with dharma, even in difficult situations, leads to inner peace and spiritual fulfillment, while selfish motives result in suffering, but redemption is always possible through righteous living.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"What Arjuna Asked Krishna about war?\"\n",
    "answer = Full_Flow(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629059f-90b8-4d3e-85a0-1335ccecbda8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ecd63-4aaa-45e0-a47c-dcf10dc12681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4897e3a7-6f44-4a9a-81da-a77cdbcd32bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7921da-29bd-4df1-8776-9fce216fc0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3acc63-78d9-4913-8c8a-279500d280e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf498f2c-e548-45cd-8672-2ba2a5c7e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     escapers= ['exit']\n",
    "#     question = input('USER : ')\n",
    "#     answer= 'MAHARAJ :' + generate_answer(question)\n",
    " \n",
    "#     if question.lower() in escapers:\n",
    "#         break\n",
    "#     for char in answer:\n",
    "#         sys.stdout.write(char)  # Write character without newline\n",
    "#         sys.stdout.flush()      # Force immediate output\n",
    "#         time.sleep(0.05)         # Adjust speed\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4eb15-d349-4184-966b-0b72f72e7827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
