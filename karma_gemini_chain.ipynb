{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c8b9e8-5684-4389-9242-32d0d24feda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import dependencies\n",
    "\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import ast\n",
    "import pandas\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from datasets import load_dataset\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bdd5c4d-de07-49f9-92c6-db402d3b774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset from HF and Convert to Pandas DataFrame\n",
    "\n",
    "# dataset = load_dataset(\"AbhirajSinghRajpurohit/karma_LLM_model_Dataset\")\n",
    "# qa_pairs = dataset[\"train\"].to_pandas()\n",
    "\n",
    "# data=pandas.read_csv('Content_Storage_df.csv')\n",
    "# Content_Storage_df = pandas.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e385d4-1d7b-42c1-bcf4-44bbdc41f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = chroma_client.get_collection(\"vector_embeddings\")\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "440ccd1b-59c6-43f7-ae2c-e721312f2821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 8190,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "retrieve_grader_1 = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash-8b\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"\"\"You are a grader assessing the relevance of a retrieved document to a user question. \n",
    "                        Give a binary score 'yes' or 'no' to indicate whether the document is relevant.\n",
    "                        Provide the binary score as JSON with a single key 'score'.\n",
    "                        input format is 'question : question , document : document'. \"\"\"\n",
    ")\n",
    "web_search_1_5= genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash-8b\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"\"\"You are an AI assistant specialized in retrieving spiritual and positive context from the Garud Puran.  \n",
    "                        Use the given question to generate a meaningful response that aligns with the teachings of the Garud Puran.  \n",
    "                        If the user asks about their **karma, actions, or deeds**, include both the **pros and cons** based on what will happen in **Swarg (heaven) and Nark (hell)** according to the Garud Puran and Sanatan Dharma.\n",
    "                        Clearly mention the **specific rewards in Swarg** for good deeds and the **specific punishments in Nark** for bad deeds, as described in the scriptures.  \n",
    "\n",
    "                        For **general queries**, include a relevant example from the Garud Puran to illustrate the concept effectively.  \n",
    "                        Ensure the response is spiritually uplifting and offers guidance in a concise format, with a maximum of three paragraphs.\n",
    "                        Each generated context should be **unique**, providing fresh insights or varying perspectives while staying true to the scripture's teachings.\n",
    "                        Use **simple and clear English**, avoiding complex words, so the response is easy to understand for all users.\n",
    "                        \"\"\"\n",
    ")\n",
    "answer_generator_2 = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-pro\",\n",
    "  generation_config=generation_config,\n",
    "  # system_instruction=\"\"\"You are an AI assistant designed for question-answering tasks.  \n",
    "  #                       Use the provided context to generate accurate and relevant answers related to garud puran in spiritual way.  \n",
    "  #                       If the answer is not found in the context, respond with \"I don't know.\"  \n",
    "  #                       Keep your response concise, with a maximum of three sentences.  \n",
    "  #                       End with a positive thought related to the question. \"\"\"\n",
    "  system_instruction= \"\"\"You are an AI assistant designed for question-answering tasks based on the Garud Puran.\n",
    "                        \n",
    "                        Use the retrieved context to generate an accurate and spiritually meaningful response.  \n",
    "                        Ensure that the answer aligns with the teachings of the Garud Puran and maintains a positive and enlightening tone.  \n",
    "                        \n",
    "                        If the user asks about their **karma, actions, or deeds**, include both the **pros and cons** based on what will happen in **Swarg (heaven) and Nark (hell)** according to the Garud Puran and Sanatan Dharma.\n",
    "                        Clearly mention the **specific rewards in Swarg** for good deeds and the **specific punishments in Nark** for bad deeds, as described in the scriptures.  \n",
    "\n",
    "                        For **general queries**, provide a relevant example from the Garud Puran to illustrate the concept effectively.\n",
    "\n",
    "                        ### **Response Structure:**  \n",
    "                        1️⃣ **First, directly answer the question and specific punishments in Nark or specific rewards in Swarg according to question.**  \n",
    "                        2️⃣ **Then, provide guidance on how to resolve or approach the problem from a spiritual perspective.**  \n",
    "                        3️⃣ **Finally, include a general spiritual thought or insight related to the question.**  \n",
    "                        \n",
    "                        Keep your response concise, with a maximum of three sentences.  \n",
    "                        End with a positive thought related to the question, inspired by the spiritual wisdom of the Garud Puran.  \n",
    "                        Use **simple and clear English**, avoiding complex words, so the response is easy to understand for all users.\n",
    "                        **Respond in the same language in which the user asks the question.**  \n",
    "                        \"\"\"\n",
    ")\n",
    "hallucination_detection_3 = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash-8b\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"\"\"You are verifying whether the model-generated answer is factually correct based on the provided context.  \n",
    "                        If the answer includes information not found in the context, classify it as hallucinated.  \n",
    "                        Respond with a JSON object containing a single key `\"hallucination\"`, with a value of `\"yes\"` or `\"no\"`.  \n",
    "                        \n",
    "                        Output Format:  \n",
    "                        {\n",
    "                          \"hallucination\": \"yes\"  // If the answer contains hallucinated information  \n",
    "                        }  \n",
    "                        {\n",
    "                          \"hallucination\": \"no\"   // If the answer is fully supported by the context  \n",
    "                        }   \"\"\"\n",
    ")\n",
    "question_resolving_detection_4 = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash-8b\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"\"\"You are a grader evaluating whether an answer is useful in resolving the given question.  \n",
    "                        Assess if the answer is relevant, clear, and provides sufficient information to address the question.  \n",
    "                        Respond with a JSON object containing a single key `\"score\"`, with a value of `\"yes\"` or `\"no\"`.  \n",
    "                        \n",
    "                        Input Format:  \n",
    "                        question: {question}, answer: {answer}  \n",
    "                        \n",
    "                        Output Format:  \n",
    "                        {\n",
    "                          \"score\": \"yes\"  // If the answer is useful  \n",
    "                        }  \n",
    "                        {\n",
    "                          \"score\": \"no\"   // If the answer is not useful  \n",
    "                        }  \n",
    "                        \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5c7deec-3600-4fe5-ae96-5fd5f4f5a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, top_k=3):\n",
    "    embedding_fn = embedding_functions.DefaultEmbeddingFunction()\n",
    "    question_embedding = embedding_fn([question])[0]\n",
    "\n",
    "    # Retrieve top-k matching documents\n",
    "    results = collection.query(\n",
    "        query_embeddings=[question_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    if results[\"documents\"]:\n",
    "        # print(results[\"documents\"])\n",
    "        flat_documents = [doc for sublist in results[\"documents\"] for doc in sublist]\n",
    "        return \" \".join(flat_documents) if flat_documents else \"No relevant context found.\"\n",
    "    \n",
    "    return \"No relevant context found.\"\n",
    "\n",
    "def retrieve_grader_function(question):\n",
    "    \n",
    "    chat_session = retrieve_grader_1.start_chat(\n",
    "                history=history\n",
    "            )\n",
    "    \n",
    "    response = chat_session.send_message(question)\n",
    "    \n",
    "    model_response=response.text\n",
    "    return model_response\n",
    "\n",
    "# Web search function \n",
    "def web_search(query, num_results=3):\n",
    "    \n",
    "    chat_session = web_search_1_5.start_chat(\n",
    "                history=history\n",
    "            )\n",
    "    \n",
    "    response = chat_session.send_message(question)\n",
    "    \n",
    "    model_response=response.text\n",
    "    return model_response\n",
    "    \n",
    "\n",
    "def answer_generator_function(question):\n",
    "    \n",
    "    chat_session = answer_generator_2.start_chat(\n",
    "                history=history\n",
    "            )\n",
    "    \n",
    "    response = chat_session.send_message(question)\n",
    "    \n",
    "    model_response=response.text\n",
    "    return model_response\n",
    "    \n",
    "def hallucination_detection_function(question):\n",
    "    \n",
    "    chat_session = hallucination_detection_3.start_chat(\n",
    "                history=history\n",
    "            )\n",
    "    \n",
    "    response = chat_session.send_message(question)\n",
    "    \n",
    "    model_response=response.text\n",
    "    return model_response\n",
    "def question_resolving_detection_function(question):\n",
    "    \n",
    "    chat_session = question_resolving_detection_4.start_chat(\n",
    "                history=history\n",
    "            )\n",
    "    \n",
    "    response = chat_session.send_message(question)\n",
    "    \n",
    "    model_response=response.text\n",
    "    return model_response\n",
    "\n",
    "\n",
    "#Full Path\n",
    "def Full_Flow(question):\n",
    "    flag=0\n",
    "    while True:\n",
    "        if flag==0:\n",
    "            document = retrieve_context(question)\n",
    "            model_input = f\"question : {question} , document : {document}\"\n",
    "            output = retrieve_grader_function(model_input)\n",
    "            \n",
    "            if 'yes' in output:\n",
    "                print('document found in database')\n",
    "            elif 'no' in output:\n",
    "                print('searching web.....')\n",
    "                print('document found on web.....')\n",
    "                document = web_search(question)\n",
    "        elif flag==1:\n",
    "            print('searching web.....')\n",
    "            print('document found on web.....')\n",
    "            document = web_search(question)\n",
    "        # print(document)\n",
    "        while True:   \n",
    "            #Generation of answer based on context\n",
    "            model_input = f\"question : {question},context : {document}\"\n",
    "            answer = answer_generator_function(model_input)\n",
    "            print('answer fetched from document')\n",
    "        \n",
    "            #Hallucination detection to check the correctness of answer\n",
    "            hallucination_check_input = f\"context : {document}, answer : {answer}\"\n",
    "            hallucination_output = hallucination_detection_function(hallucination_check_input)\n",
    "            if 'yes' in hallucination_output:\n",
    "                print('hallucination detected.')\n",
    "                print('regenerating the answer....\\n')\n",
    "                continue\n",
    "            elif 'no' in hallucination_output:\n",
    "                print('no hallucination detected')\n",
    "                question_resolver_input = f' question: {question}, answer: {answer}'\n",
    "                question_resolver_output = question_resolving_detection_function(question_resolver_input)\n",
    "                # print(question_resolver_output)\n",
    "                break\n",
    "            else: return None\n",
    "        if 'no' in question_resolver_output:\n",
    "            print('the generated answer do not resolve the query\\n')\n",
    "            print('searching the relevant document again.....\\n')\n",
    "            flag=1\n",
    "            continue\n",
    "            \n",
    "        elif 'yes' in question_resolver_output:\n",
    "            print('generated answer will resolve the query\\n\\n')\n",
    "            return 'answer :'+answer\n",
    "        else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "123ddd12-ec7f-4a99-9ad6-fe64780a3802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching web.....\n",
      "document found on web.....\n",
      "answer fetched from document\n",
      "no hallucination detected\n",
      "generated answer will resolve the query\n",
      "\n",
      "\n",
      "answer :Killing a dog creates negative karma, leading to potential suffering in future lives and hindering spiritual growth, while showing kindness to animals brings positive karma and increases happiness in the afterlife.  Practice compassion towards all beings to elevate your spiritual journey and avoid negative consequences.  Every act of kindness, no matter how small, contributes to a brighter future, both in this life and the next.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history=[]\n",
    "\n",
    "question = \"what will happen if i killed a dog?\"\n",
    "answer = Full_Flow(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629059f-90b8-4d3e-85a0-1335ccecbda8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ecd63-4aaa-45e0-a47c-dcf10dc12681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4897e3a7-6f44-4a9a-81da-a77cdbcd32bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7921da-29bd-4df1-8776-9fce216fc0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3acc63-78d9-4913-8c8a-279500d280e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf498f2c-e548-45cd-8672-2ba2a5c7e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     escapers= ['exit']\n",
    "#     question = input('USER : ')\n",
    "#     answer= 'MAHARAJ :' + generate_answer(question)\n",
    " \n",
    "#     if question.lower() in escapers:\n",
    "#         break\n",
    "#     for char in answer:\n",
    "#         sys.stdout.write(char)  # Write character without newline\n",
    "#         sys.stdout.flush()      # Force immediate output\n",
    "#         time.sleep(0.05)         # Adjust speed\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4eb15-d349-4184-966b-0b72f72e7827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
